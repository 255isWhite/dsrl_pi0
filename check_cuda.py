import torch
import subprocess

print("==== PyTorch / CUDA ç¯å¢ƒæ£€æŸ¥ ====")

# PyTorch ç‰ˆæœ¬
print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"PyTorch ç¼–è¯‘æ—¶ CUDA ç‰ˆæœ¬: {torch.version.cuda}")
print(f"æ˜¯å¦æ£€æµ‹åˆ° GPU: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"GPU æ•°é‡: {torch.cuda.device_count()}")
    print(f"å½“å‰ GPU: {torch.cuda.get_device_name(0)}")
    print(f"CUDA runtime ç‰ˆæœ¬(é€šè¿‡PyTorch): {torch.version.cuda}")

# NVIDIA é©±åŠ¨ & CUDA runtime
try:
    nvidia_smi = subprocess.check_output(["nvidia-smi"], encoding="utf-8")
    print("\n=== nvidia-smi è¾“å‡º ===")
    print("\n".join(nvidia_smi.splitlines()[:10]))  # åªæ‰“å°å‰10è¡Œ
except Exception as e:
    print("nvidia-smi æ— æ³•è¿è¡Œ:", e)


import jax, jaxlib, platform, subprocess

print("ğŸ Python:", platform.python_version())
print("ğŸ“¦ jax:", jax.__version__)
print("ğŸ“¦ jaxlib:", jaxlib.__version__)

# CUDA / cuDNN ç‰ˆæœ¬
print("ğŸŸ¢ jaxlib CUDA version:", getattr(jaxlib, "cuda_version", "N/A"))
print("ğŸŸ¢ jaxlib cuDNN version:", getattr(jaxlib, "cudnn_version", "N/A"))

# GPU ä¿¡æ¯
print("ğŸ’» JAX devices:", jax.devices())

